# --- bootstrap Spark + Sedona ----------------------------------------------
from pyspark.sql import SparkSession
import pyspark

# déterminer automatiquement la bonne paire Spark / Scala
spark_mm  = '.'.join(pyspark.__version__.split('.')[:2])        # "3.4", "3.5", …
scala_ver = '2.13' if float(spark_mm) >= 3.5 else '2.12'

WAREHOUSE_DIR = "/home/arthur/Benchmark-Tests/cycling spark-warehouse" # en dur pour l'instant mais doit etre parametrable

sedona_version = "1.7.2"
sedona_pkg   = f"org.apache.sedona:sedona-spark-{spark_mm}_{scala_ver}:{sedona_version}"
geotools_pkg = f"org.datasyslab:geotools-wrapper:{sedona_version}-28.5"

try:
    spark
except NameError:
    spark = (
        SparkSession.builder
        .appName("sedona_auto")
        .config("spark.jars.packages", f"{sedona_pkg},{geotools_pkg}")
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        .config("spark.kryo.registrator",
                "org.apache.sedona.core.serde.SedonaKryoRegistrator")
        .config("spark.sql.warehouse.dir", WAREHOUSE_DIR)
        .config("spark.sql.catalogImplementation", "hive")
        .enableHiveSupport()    
        .getOrCreate()
    )

try:
    from sedona.spark import SedonaContext
    SedonaContext.create(spark)
except Exception:
    from sedona.register import SedonaRegistrator
    SedonaRegistrator.registerAll(spark)
# ---------------------------------------------------------------------------
# Auto-generated Sedona query — DO NOT EDIT BY HAND

df = None  # Placeholder to avoid NameError if joins exist
df = spark.read.table("berlin_districts")
df.createOrReplaceTempView("d")
df.createOrReplaceTempView("berlin_districts")
df_r = spark.read.table("ride_points")
df_r.createOrReplaceTempView("r")
df_r.createOrReplaceTempView("ride_points")
df = spark.sql("""
    SELECT d.name AS `d.name`, r.trip_id AS `r.trip_id`
    FROM d
    LEFT JOIN r
    ON (ST_Intersects(ST_GeomFromText('POLYGON((13.3 52.45, 13.5 52.45, 13.5 52.58, 13.3 52.58, 13.3 52.45))', 4326), ST_Point(r.x, r.y)) AND (r.t BETWEEN '2022-10-04T18:00:00Z' AND '2022-10-04T19:00:00Z'))
""")
from pyspark.sql import functions as F
df = df.filter("`d.name` IS NOT NULL")
df = df.groupBy(F.col("`d.name`")).agg(F.countDistinct(F.col("`r.trip_id`")).alias("n_trips"))
from pyspark.sql import functions as F
df = df.orderBy(F.col("n_trips").desc())
df = df.limit(10)
result = df

# Final result variable is: result
result.show()
